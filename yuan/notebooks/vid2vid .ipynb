{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a jupyter notebook for the vid2vid algorithm\n",
    "`https://github.com/NVIDIA/vid2vid`\n",
    "\n",
    "Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm uses pytorch instead of tensorflow, which is what pix2pix uses. However it allows the user to use tensorboard, to evaluate the results of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite for Vid2Vid:\n",
    "- Python 3\n",
    "- NVIDIA GPU + CUDA cuDNN\n",
    "- PyTorch 0.4 ** still permission for **\n",
    "- Python libraries dominate and requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues while running the code, while downloading the \n",
    "\n",
    "The placement of the download_gdrive.py is outside of the scripts folder, and it raises an error when one of the scripts inside the scripts folder tries to import it. \n",
    "    --> to solve this problem, we can move teh download_gdrive.py script inside the scripts directory and edit the script to import from its current directory.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA\n",
    "This is a plateform that is developed by NVIDIA to perform parallel computing, however I have found a script that allows to use just one CPU but, the results are not garanteed to be as good as using 8 GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using just one GPU with vid2vid\n",
    "\n",
    "`We trained our models using multiple GPUs. For convenience, we provide some sample training scripts (train_g1_XXX.sh) for single GPU users, up to 1024 x 512 resolution. Again a coarse-to-fine approach is adopted (256 x 128, 512 x 256, 1024 x 512). Performance is not guaranteed using these scripts.`\n",
    "\n",
    "For example, to train a 256 x 128 video with a single GPU (bash ./scripts/street/train_g1_256.sh)\n",
    "`python train.py --name label2city_256_g1 --label_nc 35 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things we can when training with one GPU and is hitting memeory errors:\n",
    "\n",
    "- set the `n_gpus_gen` flag to -1 so that there is no seperatration and all GPUs are used for both discriminator and generators --> only works for low res images\n",
    "- Decrease the `max_frames_per_gpu`, the default number is 1.\n",
    "- Increasing `max_frames_backpropagate` will slightly improve performance, but trainning wil also be less stable.\n",
    "- `n_frames_total` gets gradually increased during training.\n",
    "- `no_first_img` if not specified, the model will assume the first frame is given and synthesize the successive frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training details\n",
    "\n",
    "We generate frames in the video sequentially, where the generation of the current frame depends on previous frames. To generate the first frame for the model, there are 3 different ways:\n",
    "\n",
    "        Using another generator which was trained on generating single images (e.g., pix2pixHD) by specifying --use_single_G. This is the option we use in the test scripts.\n",
    "        Using the first frame in the real sequence by specifying --use_real_img.\n",
    "        Forcing the model to also synthesize the first frame by specifying --no_first_img. This must be trained separately before inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
